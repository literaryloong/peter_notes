## 整体结构

初步设计的项企数据中台框架，其中：

+ `DataHub` 元数据管理平台；定时抽取PG原数据库的表结构信息
+ `Flink` 数据计算平台，主要负责数据抽取和流转过程的ETL
+ `IceBerg` 数据湖事务层
+ `DM`层使用关系型数据库或列式数据库加速查询效率
+ `DolphinScheduler` 任务调度器(考虑需要实现定时任务)
+ `Zeppelin web notebook`工具，可以执行SQL和Flink查询
+ `Ceph` 提供类似S3接口的文档存储服务

![image-20220407092559439](imgs/项企数据中台实践/image-20220407092559439.png)



主要功能：

+ 元数据管理  ----- DataHub
  + 自动提取
  + 搜索
  + 标签
  + 权限*
  + 血缘*
+ FlinkSQL提交执行  ----  Dlink
  + sql在线编写
  + sql提交到flink集群
  + sql任务管理
  + sql任务调度
+ 自助取数分析
  + 支持sql在线查询，列表图形展示
  + 自动发布api*



## ODS数据导入

```sql
workflow_action_journal 操作记录
workflow_button_auth
workflow_comment 留言
workflow_form_fields 表单定义项说明
workflow_group 流程分类（appid=2用户流程  parent_id null的大分类）
workflow_process 流程定义的具体名称
workflow_process_achievements 流程每个任务节点的起止操作时间，记录操作处理时间
workflow_process_form 表单
workflow_process_form_draft 表单草稿
workflow_process_history 历史表达数据
workflow_process_page 无数据
workflow_process_relation
workflow_process_version 激励procdef_id proc_key与process-id的关系，
```



```shell
emis0322.public.notice_popup 通知弹框
emis0322.public.operation_log ES数据
emis0322.public.workflow_action_journal 批阅处理
emis0322.public.workflow_comment 备注
emis0322.public.workflow_process_form_draft 表单草稿
emis0322.public.workflow_process_relation 流程关联人记录表
emis0322.public.workflow_process_version 流程配置版本信息表

workflow0322.public.act_hi_comment 历史流程审批意见表
workflow0322.public.act_hi_detail 历史的流程运行详情表
```



```sql
# 查看当前
select * from pg_replication_slots;
# 删除槽
select * from  pg_drop_replication_slot('camundaworkflow_ch_replication_slot');
select * from  pg_drop_replication_slot('video_access_ch_replication_slot');
select * from  pg_drop_replication_slot('flink_xq1_workflow_process_form_draft');
select * from  pg_drop_replication_slot('flink_xq1_workflow_process_relation');
select * from  pg_drop_replication_slot('flink_xq1_workflow_process_version');
select * from  pg_drop_replication_slot('flink_xq1_operation_log');
select * from  pg_drop_replication_slot('flink_xq1_notice_popup');

```



> 创建新的slot可以重新捞取全量数据



## Flink-Iceberg

开发的一款类似`datax`的数据导入工具，基于Flink。

+ Sources
  + `postgresql` 全库同步，支持批量和`cdc`两种
  + ES 批量，支持按条件查询
+ Sink
  + `Clickhouse`
  + Iceberg



构建：

```sh
mvn install -Panxinyun
```

> 项目中`flink-connector-jdbc`项目为`flink`官方1.13release本地fork，没有作为子模块加入。如有改动，需要先本地构建



执行Flink-Iceberg程序：

![image-20220526162125844](imgs/项企数据中台实践/image-20220526162125844.png)



参数说明：

1. 主类：`com.fs.Main`
2. 参数：`job.config=pgdump-pg2iceberg.yml`



参数例子：

```yaml
set:
  title: EMIS-CDC-ICEBERG
  execution.checkpointing.interval: 300

source:
  type: postgres-cdc
  config:
    host_port: 10.8.30.156:5432
    database: emis0430
    # Credentials
    username: postgres
    password: postgres
    debezium.slot.name: flink_xq
    debezium.plugin.name: wal2json
    debezium.slot.drop.on.stop: true
    tables: api_log(user_no),notice_popup,operation_log,workflow_action_journal,workflow_comment,workflow_process_form_draft,workflow_process_relation,workflow_process_version

sink:
  type: "iceberg"
  config:
    desc: nothingToSay
  catalog:
    name: iceberg_flink
    type: iceberg
    default-database: ib
    config:
      catalog-type: hadoop
      warehouse: hdfs://10.8.30.37:8020/user/hadoop
      property-version: 1
```



提交时报错啦：

```
org.apache.flink.runtime.rest.handler.RestHandlerException: Could not execute application. 
at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$handleRequest$1(JarRunHandler.java:110) 
at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) 
at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) 
at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) 
at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1609) 
at java.lang.Thread.run(Thread.java:750) Caused by: java.util.concurrent.CompletionException: java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration 
at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) 
at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) 
at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1606) ... 1 more Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration 
at org.apache.iceberg.flink.FlinkCatalogFactory.clusterHadoopConf(FlinkCatalogFactory.java:160) 
at org.apache.iceberg.flink.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:126) 
at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:244) 
at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1243) 
at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1106) 
at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:730) 
at com.fs.catalogs.IceBergCatalog.registry(IceBergCatalog.scala:59) 
at com.fs.jobmanager.JobManager$.flinkRun(JobManager.scala:166) 
at com.fs.jobmanager.JobManager$.start(JobManager.scala:333) 
at com.fs.Main$.main(Main.scala:43) 
at com.fs.Main.main(Main.scala) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.lang.reflect.Method.invoke(Method.java:498) 
at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) 
at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) 
at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114) 
at org.apache.flink.client.deployment.application.DetachedApplicationRunner.tryExecuteJobs(DetachedApplicationRunner.java:84) 
at org.apache.flink.client.deployment.application.DetachedApplicationRunner.run(DetachedApplicationRunner.java:70) 
at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$handleRequest$0(JarRunHandler.java:104) 
at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) ... 1 more Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.conf.Configuration 
at java.net.URLClassLoader.findClass(URLClassLoader.java:387) 
at java.lang.ClassLoader.loadClass(ClassLoader.java:418) 
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352) 
at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ... 23 more
```



老朋友了：`ClassNotFoundException: org.apache.hadoop.conf.Configuration`



如果大家是使用idea+maven进行开发的话，可以到pom.xml文件，点击`alt+insert`快捷键，选择第一个`Dependency`

![image-20220531160036165](imgs/项企数据中台实践/image-20220531160036165.png)

但是已经引用：

![image-20220531160058366](imgs/项企数据中台实践/image-20220531160058366.png)



那问题就是：`包冲突了`  钝角...

猜想：`pom`中引用的`hadoop-common`其实无效，已被其他的`flink`相关包中的覆盖。而打包发布的时候，`flink`相关包是provide范围的。



```sh
mvn dependency:tree -Panxinyun > ww
```

![image-20220531161031238](imgs/项企数据中台实践/image-20220531161031238.png)



正常编译到包里了！？



进一步：

问题说明：在独立部署的`flink-session`中启动没有报错，在k8s部署的`flink`环境中出错

查看独立部署的`flink`的lib，发下有两个`hadoop`相关的库：



```
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-hadoop-compatibility_2.11</artifactId>
    <version>${flink.version}</version>
</dependency>

<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-shaded-hadoop-2-uber</artifactId>
    <version>2.7.5-10.0</version>
</dependency>
```



还是不行。。。



在网上找到一条说：

 ![image-20220531162811547](imgs/项企数据中台实践/image-20220531162811547.png)

需要打包包含`hadoop`的Flink容器。 TODO



解决37上执行OOM的问题：

### OutOfMemoryError: Java heap space [#](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_trouble/#outofmemoryerror-java-heap-space)

The exception usually indicates that the *JVM Heap* is too small. You can try to increase the JVM Heap size by increasing [total memory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup/#configure-total-memory). You can also directly increase [task heap memory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/#task-operator-heap-memory) for TaskManagers or [JVM Heap memory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_jobmanager/#configure-jvm-heap) for JobManagers.

> You can also increase the [framework heap memory](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/mem_setup_tm/#framework-memory) for TaskManagers, but you should only change this option if you are sure the Flink framework itself needs more memory.

![Flink's process memory model](https://nightlies.apache.org/flink/flink-docs-master/fig/process_mem_model.svg)



| **Component**        | **Option for TaskManager**                                   | **Option for JobManager**                                    |
| :------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| Total Flink memory   | [`taskmanager.memory.flink.size`](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#taskmanager-memory-flink-size) | [`jobmanager.memory.flink.size`](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#jobmanager-memory-flink-size) |
| Total process memory | [`taskmanager.memory.process.size`](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#taskmanager-memory-process-size) | [`jobmanager.memory.process.size`](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#jobmanager-memory-process-size) |



修改参数（将taskmanager.memory.flink.size放大到4G）后重启flink集群，重新提交任务执行，成功



流式查询：

![image-20220531171935183](imgs/项企数据中台实践/image-20220531171935183.png)



写入Iceberg报错

> some thing went wrong!!! org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto cannot be cast to com.google.protobuf.Message

没找到问题，把hdfs上面之前存入的iceberg目录删除了，重新运行





## DataHub

### 概述

Datahub是一个现代化的数据目录，支持端到端的数据发现、数据可观察、数据治理。这个可扩展的元数据平台使开发者能管理不断更新的数据生态系统，数据分析师能够充分利用组织内部的数据。

> DataHub is a modern data catalog built to enable end-to-end data discovery, data observability, and data governance. This extensible metadata platform is built for developers to tame the complexity of their rapidly evolving data ecosystems, and for data practitioners to leverage the full value of data within their organization.

![在这里插入图片描述](imgs/项企数据中台实践/a2e2e74b8468b441d2010fd8269d0355.JPEG)

- datahub-gms: Metadata store service
- datahub-frontend: Play application, which serves DataHub frontend
- datahub-mce-consumer: Kafka Streams application that consumes from
  Metadata Change Event (MCE) stream and updates metadata store
- datahub-mae-consumer: Kafka Streams application that consumes from
  Metadata Audit Event (MAE) stream and builds search index and graph
  db



文档：https://datahubproject.io/docs/metadata-ingestion/

用向导创建 `Ingestion`

![image-20220318090147973](imgs/项企数据中台实践/image-20220318090147973.png)

新增ingestion source:

![image-20220318090225919](imgs/项企数据中台实践/image-20220318090225919.png)

配置后执行run：

报错

```sh
'ConfigurationError: Unable to connect to http://10.8.30.184:9002/api/gms/config with status_code: 401. Maybe you need to set up '
'authentication? Please check your configuration and make sure you are talking to the DataHub GMS (usually <datahub-gms-host>:8080) or '
'Frontend GMS API (usually <frontend>:9002/api/gms).\n'
```



基于推送的摄取可以使用预构建的发射器，也可以使用我们的框架发出自定义事件。

基于拉取的摄取爬取元数据源。我们已经与 Kafka、MySQL、MS SQL、Postgres、LDAP、Snowflake、Hive、BigQuery 等进行了预构建集成。可以使用我们的 Airflow 集成或其他选择的调度程序来自动化摄取。



### 导入数据[Data Ingestion]

```shell
pip install 'acryl-datahub[datahub-rest]'
pip install 'acryl-datahub[mysql]'
pip install 'acryl-datahub[postgres]'

# recipe.yml
```

```yml
source:
  type: "mysql"
  config:
    username: "root"
    password: "123456"
    host_port: "10.8.30.37:3306"
sink:
  type: "datahub-rest"
  config:
    server: 'http://10.8.30.184:8080'
```

```sh
datahub ingest -c recipe.yml
```

导入项企数据：

```yaml
source:
  type: postgres
  config:
    # Coordinates
    host_port: 10.8.30.156:5432
    database: emis0322

    # Credentials
    username: postgres
    password: postgres

    # Options
    database_alias: emis
    
sink:
  type: "datahub-rest"
  config:
    server: 'http://10.8.30.184:8080'
```



### Great Expectations

通过Great Expectations中的`DataHubValidationAction`对数据质量进行评估。

```
pip install 'acryl-datahub[great-expectations]'
```



需要在Great Expectations的Checkpoint的action_list中增加：

```yaml
action_list:
  - name: datahub_action
    action:
      module_name: datahub.integrations.great_expectations.action
      class_name: DataHubValidationAction
      server_url: http://localhost:8080 #datahub server url
```

关于Great Expectations的配置[Checkpoints and Actions](https://docs.greatexpectations.io/docs/reference/checkpoints_and_actions/);



datahub界面中将展示Assertion状态

![image-20220324090832779](imgs/项企数据中台实践/image-20220324090832779.png)



### 血统系统 Lineage

使用Airflow作为血缘系统的后端

```sh
pip install acryl-datahub[airflow]

airflow connections add  --conn-type 'datahub_rest' 'datahub_rest_default' --conn-host 'http://localhost:8080'

```

`airflow.cfg`

```yaml
[lineage]
backend = datahub_provider.lineage.datahub.DatahubLineageBackend
datahub_kwargs = {
    "datahub_conn_id": "datahub_rest_default",
    "cluster": "prod",
    "capture_ownership_info": true,
    "capture_tags_info": true,
    "graceful_exceptions": true }
```



## 数据导入

> PREPARE
>
> ```shell
> #清理空间
> hdfs dfs -rm -R -skipTrash /warehouse
> ```
>
> 

