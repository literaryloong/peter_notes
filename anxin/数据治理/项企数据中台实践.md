## 整体结构

初步设计的项企数据中台框架，其中：

+ `DataHub` 元数据管理平台；定时抽取PG原数据库的表结构信息
+ `Flink` 数据计算平台，主要负责数据抽取和流转过程的ETL
+ `IceBerg` 数据湖事务层
+ `DM`层使用关系型数据库或列式数据库加速查询效率
+ `DolphinScheduler` 任务调度器(考虑需要实现定时任务)
+ `Zeppelin web notebook`工具，可以执行SQL和Flink查询
+ `Ceph` 提供类似S3接口的文档存储服务

![image-20220407092559439](imgs/项企数据中台实践/image-20220407092559439.png)



主要功能：

+ 元数据管理  ----- DataHub
  + 自动提取
  + 搜索
  + 标签
  + 权限*
  + 血缘*
+ FlinkSQL提交执行  ----  Dlink
  + sql在线编写
  + sql提交到flink集群
  + sql任务管理
  + sql任务调度
+ 自助取数分析
  + 支持sql在线查询，列表图形展示
  + 自动发布api*



## DataHub

### 概述

Datahub是一个现代化的数据目录，支持端到端的数据发现、数据可观察、数据治理。这个可扩展的元数据平台使开发者能管理不断更新的数据生态系统，数据分析师能够充分利用组织内部的数据。

> DataHub is a modern data catalog built to enable end-to-end data discovery, data observability, and data governance. This extensible metadata platform is built for developers to tame the complexity of their rapidly evolving data ecosystems, and for data practitioners to leverage the full value of data within their organization.

![在这里插入图片描述](imgs/项企数据中台实践/a2e2e74b8468b441d2010fd8269d0355.JPEG)

- datahub-gms: Metadata store service
- datahub-frontend: Play application, which serves DataHub frontend
- datahub-mce-consumer: Kafka Streams application that consumes from
  Metadata Change Event (MCE) stream and updates metadata store
- datahub-mae-consumer: Kafka Streams application that consumes from
  Metadata Audit Event (MAE) stream and builds search index and graph
  db



文档：https://datahubproject.io/docs/metadata-ingestion/

用向导创建 `Ingestion`

![image-20220318090147973](imgs/项企数据中台实践/image-20220318090147973.png)

新增ingestion source:

![image-20220318090225919](imgs/项企数据中台实践/image-20220318090225919.png)

配置后执行run：

报错

```sh
'ConfigurationError: Unable to connect to http://10.8.30.184:9002/api/gms/config with status_code: 401. Maybe you need to set up '
'authentication? Please check your configuration and make sure you are talking to the DataHub GMS (usually <datahub-gms-host>:8080) or '
'Frontend GMS API (usually <frontend>:9002/api/gms).\n'
```



基于推送的摄取可以使用预构建的发射器，也可以使用我们的框架发出自定义事件。

基于拉取的摄取爬取元数据源。我们已经与 Kafka、MySQL、MS SQL、Postgres、LDAP、Snowflake、Hive、BigQuery 等进行了预构建集成。可以使用我们的 Airflow 集成或其他选择的调度程序来自动化摄取。



### 导入数据[Data Ingestion]

```shell
pip install 'acryl-datahub[datahub-rest]'
pip install 'acryl-datahub[mysql]'
pip install 'acryl-datahub[postgres]'

# recipe.yml
```

```yml
source:
  type: "mysql"
  config:
    username: "root"
    password: "123456"
    host_port: "10.8.30.37:3306"
sink:
  type: "datahub-rest"
  config:
    server: 'http://10.8.30.184:8080'
```

```sh
datahub ingest -c recipe.yml
```

导入项企数据：

```yaml
source:
  type: postgres
  config:
    # Coordinates
    host_port: 10.8.30.156:5432
    database: emis0322

    # Credentials
    username: postgres
    password: postgres

    # Options
    database_alias: emis
    
sink:
  type: "datahub-rest"
  config:
    server: 'http://10.8.30.184:8080'
```



### Great Expectations

通过Great Expectations中的`DataHubValidationAction`对数据质量进行评估。

```
pip install 'acryl-datahub[great-expectations]'
```



需要在Great Expectations的Checkpoint的action_list中增加：

```yaml
action_list:
  - name: datahub_action
    action:
      module_name: datahub.integrations.great_expectations.action
      class_name: DataHubValidationAction
      server_url: http://localhost:8080 #datahub server url
```

关于Great Expectations的配置[Checkpoints and Actions](https://docs.greatexpectations.io/docs/reference/checkpoints_and_actions/);



datahub界面中将展示Assertion状态

![image-20220324090832779](imgs/项企数据中台实践/image-20220324090832779.png)



### 血统系统 Lineage

使用Airflow作为血缘系统的后端

```sh
pip install acryl-datahub[airflow]

airflow connections add  --conn-type 'datahub_rest' 'datahub_rest_default' --conn-host 'http://localhost:8080'

```

`airflow.cfg`

```yaml
[lineage]
backend = datahub_provider.lineage.datahub.DatahubLineageBackend
datahub_kwargs = {
    "datahub_conn_id": "datahub_rest_default",
    "cluster": "prod",
    "capture_ownership_info": true,
    "capture_tags_info": true,
    "graceful_exceptions": true }
```



## 数据导入

> PREPARE
>
> ```shell
> #清理空间
> hdfs dfs -rm -R -skipTrash /warehouse
> ```
>
> 

